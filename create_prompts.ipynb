{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d0665f",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Loading the FEVER dataset](#Loading-the-FEVER-dataset)\n",
    "1. [Using an internal company LLM API (Falcon-40B) to get assumptions](#Falcon-40B-Offering-to-get-assumptions)\n",
    "1. [Using ReAct to get reasoning chains](#Using-ReAct-to-get-reasoning-chains)\n",
    "1. [Using Open AI API for reasoning chains](#Using-Open-AI-API)\n",
    "    1. [Prompt version 1](#Prompt-version-1)\n",
    "    1. [Prompt version 2](#Prompt-version-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb7bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('secrets.json') as f:\n",
    "    secrets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f1bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9105523",
   "metadata": {},
   "source": [
    "## Loading the FEVER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537d1805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deanorenstein/opt/anaconda3/envs/nlu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2666f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = load_dataset(\"fever\", \"v1.0\") # training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val = load_dataset(\"fever\", \"v2.0\") # validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daed2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_claim = d_train['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d7af11c-7b45-486b-95eb-d22540431c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 75397,\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.',\n",
       " 'evidence_annotation_id': 92206,\n",
       " 'evidence_id': 104971,\n",
       " 'evidence_wiki_url': 'Nikolaj_Coster-Waldau',\n",
       " 'evidence_sentence_id': 7}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d22940",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_with_evidence = d_train['train'][1059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_wiki = load_dataset(\"fever\", \"wiki_pages\") # wikipedia evidence\n",
    "d_wikipages = d_wiki['wikipedia_pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3de0eebe-d8c8-4003-bbf6-6ad0e93c4ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1974_in_Cambodia',\n",
       " 'text': 'The following lists events that happened during 1974 in Cambodia . ',\n",
       " 'lines': '0\\tThe following lists events that happened during 1974 in Cambodia .\\t1974\\t1974\\tCambodia\\tCambodia\\n1\\t'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_wikipages[104971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text = {val: text for val, text in zip(d_wikipages['id'], d_wikipages['text'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f311d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fever/wiki_text.pkl', 'wb') as f:\n",
    "    pickle.dump(wiki_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc33a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fever/wiki_text.pkl', 'rb') as f:\n",
    "    wiki_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11150a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "bad_pattern_1 = re.compile(r'\\s*-RCB-\\s*')\n",
    "bad_css = ['align = ', 'width = ', 'colspan = ', 'style = ', 'bgcolor = ']\n",
    "cleaned_wiki_text = {}\n",
    "#Removing any CSS properties or unneeded phrases/characters from the wikipedia evidence (so we have less chance of exceeding the context window)\n",
    "m = 0\n",
    "for v, t in wiki_text.items():\n",
    "    l = len(t)\n",
    "\n",
    "    temp = ''\n",
    "    for token in t.split('|'):\n",
    "        token = re.sub(bad_pattern_1, ' ', token)\n",
    "        if token == ' ' or token == ' -  ' or any(pat in token for pat in bad_css):\n",
    "            continue\n",
    "\n",
    "        temp += token\n",
    "    t = temp\n",
    "    cleaned_wiki_text[v] = t\n",
    "    \n",
    "    if l > m:\n",
    "        m = l\n",
    "        x = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da1fe3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text = cleaned_wiki_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ba3fb",
   "metadata": {},
   "source": [
    "Load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27235c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 113501,\n",
       " 'label': 'NOT ENOUGH INFO',\n",
       " 'claim': 'Grease had bad reviews.',\n",
       " 'evidence_annotation_id': 133128,\n",
       " 'evidence_id': -1,\n",
       " 'evidence_wiki_url': '',\n",
       " 'evidence_sentence_id': -1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train['paper_test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4a8d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "evidences = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "071e44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_claim = ''\n",
    "prev_l = ''\n",
    "curr_wiki_set = set()\n",
    "for i in range(len(d_train['paper_test'])):\n",
    "    example = d_train['paper_test'][i]\n",
    "    c, e, l = example['claim'], example['evidence_wiki_url'], example['label']\n",
    "    if e in wiki_text:\n",
    "        evidence = wiki_text[e]\n",
    "\n",
    "    if c == prev_claim:\n",
    "        curr_wiki_set.add(evidence)\n",
    "    else:\n",
    "        if len(curr_wiki_set) > 0:\n",
    "            claims.append(prev_claim)\n",
    "            evidences.append('\\n'.join(ev for ev in list(curr_wiki_set)))\n",
    "            labels.append(prev_l)\n",
    "        curr_wiki_set = set([evidence])\n",
    "\n",
    "    prev_claim = c\n",
    "    prev_l = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92fd7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_set = pd.DataFrame({\n",
    "    'claim': claims,\n",
    "    'evidence': evidences,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "651e1b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NOT ENOUGH INFO    3333\n",
       "SUPPORTS           3333\n",
       "REFUTES            3332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_set['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "668b3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_set.to_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64ccd3",
   "metadata": {},
   "source": [
    "## Falcon 40B Offering to get assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e76c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "uri, api_key = secrets[\"DELL-LLM-URI\"], secrets[\"DELL-LLM-API-KEY\"]\n",
    "model_name = \"falcon-40b-instruct\"\n",
    "def llm_api(prompt):\n",
    "    url = uri + model_name\n",
    "    payload = {\n",
    "        \"instruction\": prompt\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"api-key\": api_key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload, verify=False)\n",
    "        response.raise_for_status()\n",
    "        response = response.json()\n",
    "        generated_text = response['response'][0]['generated_text']\n",
    "        return generated_text\n",
    "    except Exception as err:\n",
    "        if response is not None:\n",
    "            print(\"Error code:\", response.status_code)\n",
    "            print(\"Error message:\", response.text)\n",
    "        else:\n",
    "            print(\"Error:\", err)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Here is a statement:\n",
    "{ex_with_evidence['claim']}\n",
    "Make a numbered list of the assumptions you made when producing the above statement.\n",
    "\"\"\"\n",
    "res = llm_api(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence = [wiki_text[url] for url in ex_with_evidence['evidence_wiki_url'] if url in wiki_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028e69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Here is a numbered list of assertions:\n",
    "{res}\n",
    "Here is an evidence passage:\n",
    "{evidence}\n",
    "For each assertion, determine whether it is SUPPORTED, REFUTED, or NOT ENOUGH INFO based on the evidence given. Give back a numbered list, each with 1 of these labels, where each number corresponds to its corresponding assertion. Also, give back a final answer based on a majority vote\n",
    "\"\"\"\n",
    "res2 = llm_api(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "19c6165e-fabc-4ab3-aed2-a8c529828ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The movie is called Sully\n",
      "2. Sully is a real person\n",
      "3. Tom Hanks is an actor\n",
      "4. Tom Hanks has played real people before\n",
      "5. Tom Hanks played Sully in a movie called Sully.\n"
     ]
    }
   ],
   "source": [
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2cbb74",
   "metadata": {},
   "source": [
    "## Using ReAct to get reasoning chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a70429",
   "metadata": {},
   "source": [
    "Seeing what the data looks like from ReAct paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6fc4b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/react/fever.json', 'r') as f:\n",
    "    x = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f7d867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['webact_simple3', 'cotqa_simple3', 'webqa_simple3', 'webthink_simple3'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x['webact_simple3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x['cotqa_simple3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x['webqa_simple3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x['webthink_simple3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db65ffc",
   "metadata": {},
   "source": [
    "Follow the same ReAct code from the original source (just using GPT-3.5 turbo instead now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1ca28cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = secrets[\"OPEN-AI-KEY\"]\n",
    "\n",
    "import wikienv, wrappers\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fdd4419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, stop=[\"\\n\"]):\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stop=stop\n",
    "    )\n",
    "\n",
    "    res = res['choices'][0]['message']['content']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d33e82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wikienv.WikiEnv()\n",
    "env = wrappers.FeverWrapper(env, split=\"train\")\n",
    "env = wrappers.LoggingWrapper(env)\n",
    "\n",
    "def step(env, action):\n",
    "    attempts = 0\n",
    "    while attempts < 10:\n",
    "        try:\n",
    "            return env.step(action)\n",
    "        except:\n",
    "            attempts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd25b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'prompts/'\n",
    "prompt_file = 'fever.json'\n",
    "with open('data/react/' + folder + prompt_file, 'r') as f:\n",
    "    prompt_dict = json.load(f)\n",
    "\n",
    "webthink_prompt = prompt_dict['webthink_simple3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5eb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(webthink_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2aa846c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "webthink_prompt = \"\"\"\n",
    "Reason about why the claim is supported, refuted, or has not enough information based on Wikipedia evidences.\n",
    "\n",
    "Claim: Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\n",
    "Label: SUPPORTS\n",
    "Thought 1: I need to search Nikolaj Coster-Waldau and find if he has worked with the Fox Broadcasting Company.\n",
    "Action 1: Search[Nikolaj Coster-Waldau]\n",
    "Observation 1: Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and producer. He graduated from the Danish National School of Performing Arts in Copenhagen in 1993,[1] and had his breakthrough role in Denmark with the film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy drama series Game of Thrones, for which he received two Primetime Emmy Award nominations for Outstanding Supporting Actor in a Drama Series.. Coster-Waldau has appeared in numerous films in his native Denmark and Scandinavia, including Headhunters (2011) and A Thousand Times Good Night (2013). In the U.S, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a detective in the short-lived Fox television series New Amsterdam (2008), and appeared in the 2009 Fox television film Virtuality, originally intended as a pilot.\n",
    "Thought 2: Because he \"appeared in the 2009 Fox television film Virtuality\", he should have worked with the Fox Broadcasting Company. So, the claim is indeed supported by the evidence.\n",
    "Action 2: Finish[done]\n",
    "\n",
    "Claim: Stranger Things is set in Bloomington, Indiana.\n",
    "Label: REFUTES\n",
    "Thought 1: I should search for Stranger Things, and see if it is set in Bloomington, Indiana.\n",
    "Action 1: Search[Stranger Things]\n",
    "Observation 1: Stranger Things is an American science fiction horror drama television series created by the Duffer Brothers. Set in the 1980s, primarily in the fictional town of Hawkins, Indiana, the series centers on a number of mysteries and supernatural events occurring around the town and their impact on an ensemble of child and adult characters. \n",
    "Thought 2: The observation says that it is set in a \"fictional town of Hawkins, Indiana\", so it is not set in Bloomington. So, the claim is indeed refuted by the evidence.\n",
    "Action 2: Finish[done]\n",
    "\n",
    "Claim: Beautiful reached number two on the Billboard Hot 100 in 2003.?\n",
    "Label: NOT ENOUGH INFO\n",
    "Thought 1: I need to search the song Beautiful and find if it reached number two on the Billboard Hot 100 in 2003.\n",
    "Action 1: Search[Beautiful]\n",
    "Observation 1: Could not find [Beautiful]. Similar: ['Beautiful', 'Beautiful, Beautiful', 'A Beautiful Mind (film)', 'Beautiful (Christina Aguilera song)', 'Life Is Beautiful'].\n",
    "Thought 2: From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to find the song.\n",
    "Action 2: Search[Beautiful (Christina Aguilera song)]\n",
    "Observation 2: \"Beautiful\" is a song recorded by American singer Christina Aguilera for her fourth studio album, Stripped (2002).\n",
    "Thought 3: It does not mention Billboard, so I need to look up \"Billboard Hot 100\" to find if it reached number two on it in 2003.\n",
    "Action 3: Lookup[Billboard Hot 100]\n",
    "Observation 3: (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the United States, where it was certified Gold for 500,000 units shipped.\n",
    "Thought 4: It only says the song peaked at number two on the Billboard Hot 100, but not if it was in 2003. I am not sure if this claim is true or not. So, the evidence is indeed not sufficient to verify the claim.\n",
    "Action 4: Finish[done]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "90f43a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 65112,\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'Mark Zuckerberg co-founded a website.',\n",
       " 'evidence_annotation_id': 81592,\n",
       " 'evidence_id': 93354,\n",
       " 'evidence_wiki_url': 'Facebook',\n",
       " 'evidence_sentence_id': 1}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train['train'][350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "71484451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(d_train['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c1be4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webthink(idx=None, prompt=webthink_prompt, to_print=False):\n",
    "    question = env.reset(idx=idx)\n",
    "    label = df_train[df_train['claim'] == question[7:]]['label'].values[0]\n",
    "\n",
    "    if to_print:\n",
    "        print(idx, question)\n",
    "    \n",
    "    prompt += question + \"\\n\"\n",
    "    prompt += \"Label: \" + label + \"\\n\"\n",
    "    #print(prompt)\n",
    "\n",
    "    n_calls, n_badcalls = 0, 0\n",
    "    for i in range(1, 8):\n",
    "        n_calls += 1\n",
    "        thought_action = llm(prompt + f\"Thought {i}:\", stop=[f\"\\nObservation {i}:\"])\n",
    "        try:\n",
    "            thought, action = thought_action.strip().split(f\"\\nAction {i}: \")\n",
    "        except:\n",
    "            #print('ohh...', thought_action)\n",
    "            n_badcalls += 1\n",
    "            n_calls += 1\n",
    "            thought = thought_action.strip().split('\\n')[0]\n",
    "            action = llm(prompt + f\"Thought {i}: {thought}\\nAction {i}:\", stop=[f\"\\n\"]).strip()\n",
    "        obs, r, done, info = step(env, action[0].lower() + action[1:])\n",
    "        obs = obs.replace('\\\\n', '')\n",
    "        step_str = f\"Thought {i}: {thought}\\nAction {i}: {action}\\nObservation {i}: {obs}\\n\"\n",
    "        prompt += step_str\n",
    "        if to_print:\n",
    "            print(step_str)\n",
    "        if done:\n",
    "            break\n",
    "    if not done:\n",
    "        obs, r, done, info = step(env, \"finish[]\")\n",
    "    if to_print:\n",
    "        print(info, '\\n')\n",
    "    info.update({'n_calls': n_calls, 'n_badcalls': n_badcalls, 'traj': prompt})\n",
    "    return r, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1a5e00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(range(3000))\n",
    "random.Random(233).shuffle(idxs)\n",
    "rs = []\n",
    "infos = []\n",
    "old_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "52da765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/react/react_output_first_1564_seed_233.pkl', 'rb') as f:\n",
    "    so_far = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a7e62abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set([x['question_idx'] for x in infos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a961c3",
   "metadata": {},
   "source": [
    "Generate examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ebd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prog = tqdm(initial=n, total=len(idxs))\n",
    "prev = len(infos)\n",
    "n = 0\n",
    "for i in tqdm(idxs):\n",
    "    if i in seen:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        r, info = webthink(i, prompt=webthink_prompt)\n",
    "    except:\n",
    "        continue\n",
    "    rs.append(info['em'])\n",
    "    infos.append(info)\n",
    "    #print(i)\n",
    "    #infos2.append(info)\n",
    "\n",
    "    # if info['answer'] != info['gt_answer']:\n",
    "    #     print(\"answers unequal at\", i)\n",
    "    \n",
    "    # if n % 5 == 0:\n",
    "    #     time.sleep(25)\n",
    "\n",
    "    #prog.update(1)\n",
    "    assert len(infos) == prev + 1\n",
    "    prev = len(infos)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ea2a5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/react_output_first_1381.pkl', 'wb') as f:\n",
    "    pickle.dump(so_far + infos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38c5982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/react_output_first_1564_seed_233.pkl', 'wb') as f:\n",
    "    pickle.dump(so_far + infos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6eb94f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/react_output_first_1077_seed_233.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5291f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for the original react prompt template\n",
    "labels = []\n",
    "claims = []\n",
    "assertions = []\n",
    "evidences = []\n",
    "question_idxs = []\n",
    "for i in infos:\n",
    "    qi = i['question_idx']\n",
    "    a, ga = i['answer'], i['gt_answer']\n",
    "    traj = i['traj']\n",
    "    assertion_num = 1\n",
    "\n",
    "    # Only process react output if it aligns with ground truth\n",
    "    if a == ga:\n",
    "        evidence = \"\"\n",
    "        claim = \"\"\n",
    "        assertion_str = \"\"\n",
    "        start = traj.rfind(\"Claim:\")\n",
    "        end = traj.rfind(\"Finish[\")\n",
    "        steps = traj[start:end].split('\\n')\n",
    "        for s in steps:\n",
    "            if s.startswith(\"Observation\"):\n",
    "                evidence += s[15:]\n",
    "            elif s.startswith(\"Action\"):\n",
    "                continue\n",
    "            elif s.startswith(\"Claim\"):\n",
    "                claim = s[7:]\n",
    "            elif s.startswith(\"Thought\"):\n",
    "                thought = s[11:]\n",
    "                if thought.startswith(\"I\"):\n",
    "                    continue\n",
    "                assertion_str += f\"{assertion_num}. {thought} \\n\"\n",
    "                assertion_num += 1\n",
    "\n",
    "        labels.append(a)\n",
    "        claims.append(claim)\n",
    "        assertions.append(assertion_str)\n",
    "        question_idxs.append(qi)\n",
    "        evidences.append(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "010a4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for the new react prompt template (forcing it to give reasoning for ground truth)\n",
    "labels = []\n",
    "claims = []\n",
    "trajectories = []\n",
    "question_idxs = []\n",
    "n = 0\n",
    "for i in infos:\n",
    "    qi = i['question_idx']\n",
    "    a, ga = i['answer'], i['gt_answer']\n",
    "    traj = i['traj']\n",
    "\n",
    "    last_claim_chain = traj.split(\"Claim: \")[-1]\n",
    "    lines = last_claim_chain.split('\\n')\n",
    "    claim = lines[0]\n",
    "    lines[1] = \"Label: \" + lines[1]\n",
    "    trajectories.append(\"\\n\".join(lines[:-4]))\n",
    "\n",
    "    labels.append(ga)\n",
    "    claims.append(claim)\n",
    "    question_idxs.append(qi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1bf65d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_react_reasoning_chains = pd.DataFrame({\n",
    "    'question_idx': question_idxs,\n",
    "    'claim': claims,\n",
    "    'label': labels,\n",
    "    #'evidence': evidences,\n",
    "    #'assertions': assertions\n",
    "    'trajectories': trajectories\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f882591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_react_reasoning_chains.to_csv('data/react/df_react_chains_1381_forced_gt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187694d",
   "metadata": {},
   "source": [
    "## Using Open AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f83e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = secrets[\"OPEN-AI-KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "69266023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    res = res['choices'][0]['message']['content']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6f31c",
   "metadata": {},
   "source": [
    "### Prompt version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "befc560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Here is a claim:\n",
    "{ex_with_evidence['claim']}\n",
    "Make a numbered list of the assumptions made when producing the above claim.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4fa0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71237db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The claim assumes that \"Sully\" is a title of a movie.\n",
      "2. It assumes that Tom Hanks is an actor who has appeared in movies.\n",
      "3. It assumes that Tom Hanks has appeared in the movie titled \"Sully.\"\n",
      "4. It assumes that there is no other movie titled \"Sully\" that does not feature Tom Hanks.\n",
      "5. It assumes that the claim refers to a specific movie and not a book, TV show, or any other form of media.\n"
     ]
    }
   ],
   "source": [
    "print(res['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8b3dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence = [wiki_text[url] for url in ex_with_evidence['evidence_wiki_url'] if url in wiki_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cb5a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Here is a numbered list of assertions:\n",
    "{res}\n",
    "Here is an evidence passage:\n",
    "{evidence}\n",
    "For each assertion, determine whether it is SUPPORTED, REFUTED, or NOT ENOUGH INFO based on the evidence given. Give back a numbered list, each with 1 of these labels, where each number corresponds to its corresponding assertion. Also, give back a final answer based on a majority vote\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc956f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dfb117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assertions given are:\n",
      "\n",
      "1. The claim assumes that \"Sully\" is a title of a movie.\n",
      "2. It assumes that Tom Hanks is an actor who has appeared in movies.\n",
      "3. It assumes that Tom Hanks has appeared in the movie titled \"Sully.\"\n",
      "4. It assumes that there is no other movie titled \"Sully\" that does not feature Tom Hanks.\n",
      "5. It assumes that the claim refers to a specific movie and not a book, TV show, or any other form of media.\n",
      "\n",
      "Based on the evidence passage provided, we can determine the following:\n",
      "\n",
      "1. SUPPORTED - The evidence passage mentions that \"Sully\" is a 2016 American biographical drama film.\n",
      "2. SUPPORTED - The evidence passage mentions that Tom Hanks stars in the film.\n",
      "3. SUPPORTED - The evidence passage mentions that Tom Hanks plays the role of Sullenberger in the film.\n",
      "4. SUPPORTED - There is no evidence provided that suggests the existence of another movie titled \"Sully\" that does not feature Tom Hanks.\n",
      "5. SUPPORTED - The evidence passage specifically refers to the movie \"Sully\" and does not mention any other form of media.\n",
      "\n",
      "Based on the majority vote, the overall answer is SUPPORTED.\n"
     ]
    }
   ],
   "source": [
    "print(res2['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2c7f4",
   "metadata": {},
   "source": [
    "### Prompt version 2\n",
    "\n",
    "This version involves providing the ground truth label and asking the model to generate reasoning, based on the evidence, for why the ground truth label is what it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e9ce0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_ex(claim, evidence, label):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following claim, evidence, and label indicating whether the\n",
    "    the claim is supported, refuted, or not having enough information.\n",
    "    Derive a bullet list of up to 8 assertions verifying the label for the claim based\n",
    "    on the given evidence. Please provide up to three lists of assertions.\n",
    "\n",
    "    Claim - {claim}\n",
    "\n",
    "    Evidence - {evidence}\n",
    "\n",
    "    Label - {label}\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c79405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we are targeting the same sample of data used by ReAct\n",
    "import random\n",
    "idxs = list(range(3000))\n",
    "random.Random(233).shuffle(idxs)\n",
    "react_generated_examples = pd.read_csv('data/react/df_react_chains_826.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "962e714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wikienv.WikiEnv()\n",
    "env = wrappers.FeverWrapper(env, split=\"train\")\n",
    "env = wrappers.LoggingWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96613b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fever/claim_evidence.pkl', 'rb') as f:\n",
    "    claim_evidence = pickle.load(f)\n",
    "with open('data/fever/claim_labels.pkl', 'rb') as f:\n",
    "    claim_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a0d4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'prompts/'\n",
    "prompt_file = 'fever.json'\n",
    "with open('data/react/' + folder + prompt_file, 'r') as f:\n",
    "    prompt_dict = json.load(f)\n",
    "\n",
    "webthink_prompt = prompt_dict['webthink_simple3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "claims = []\n",
    "assertions = []\n",
    "evidences = []\n",
    "question_idxs = []\n",
    "for i in tqdm(idxs):\n",
    "    claim = env.reset(idx=i)\n",
    "    claim = claim[7:]\n",
    "    if claim in react_generated_examples['claim'].values:\n",
    "        #print(\"sadasd\")\n",
    "        continue\n",
    "    \n",
    "    if claim in claim_evidence:\n",
    "        evidence = claim_evidence[claim]\n",
    "        label = claim_labels[claim]\n",
    "\n",
    "        prompt = get_prompt_ex(claim, evidence, label)\n",
    "        response = llm(prompt)\n",
    "\n",
    "        print(response)\n",
    "        break\n",
    "\n",
    "        assertion_str = ...\n",
    "        \n",
    "        labels.append(label)\n",
    "        claims.append(claim)\n",
    "        assertions.append(assertion_str)\n",
    "        question_idxs.append(i)\n",
    "        evidences.append(evidence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
